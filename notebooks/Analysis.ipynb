{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8116), started 4:08:27 ago. (Use '!kill 8116' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bd3697f8e1a64b23\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bd3697f8e1a64b23\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=/home/adong/hdrnet-pytorch/jho_logs/lightning_logs/version_1701084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from model_building import HDRPointwiseNN\n",
    "from dataset import HDRDataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from utils import plot_one_image,plot_batch_of_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some validation pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '/home/adong/Downloads/carlos.png'\n",
    "#image_path = '/home/adong/Downloads/Sreyas.jpg'\n",
    "image_path = 'D:\\Downloads\\oxr2ef3hqy961.jpg'\n",
    "model_path = r'E:\\Projects\\hdrnet-pytorch\\jho_logs\\lightning_logs\\version_1701084\\checkpoints\\epoch=25-step=1044.ckpt' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for HDRPointwiseNN:\n\tUnexpected key(s) in state_dict: \"low_level.conv5.conv.weight\", \"low_level.conv5.conv.bias\", \"low_level.conv5.bn.weight\", \"low_level.conv5.bn.bias\", \"low_level.conv5.bn.running_mean\", \"low_level.conv5.bn.running_var\", \"low_level.conv5.bn.num_batches_tracked\", \"low_level.conv6.conv.weight\", \"low_level.conv6.conv.bias\", \"low_level.conv6.bn.weight\", \"low_level.conv6.bn.bias\", \"low_level.conv6.bn.running_mean\", \"low_level.conv6.bn.running_var\", \"low_level.conv6.bn.num_batches_tracked\", \"global_features.conv3.conv.weight\", \"global_features.conv3.conv.bias\", \"global_features.conv3.bn.weight\", \"global_features.conv3.bn.bias\", \"global_features.conv3.bn.running_mean\", \"global_features.conv3.bn.running_var\", \"global_features.conv3.bn.num_batches_tracked\", \"global_features.conv4.conv.weight\", \"global_features.conv4.conv.bias\", \"global_features.conv4.bn.weight\", \"global_features.conv4.bn.bias\", \"global_features.conv4.bn.running_mean\", \"global_features.conv4.bn.running_var\", \"global_features.conv4.bn.num_batches_tracked\". \n\tsize mismatch for low_level.conv1.conv.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([8, 3, 3, 3]).\n\tsize mismatch for low_level.conv1.conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv1.bn.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv1.bn.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv1.bn.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv1.bn.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 8, 3, 3]).\n\tsize mismatch for low_level.conv2.conv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv2.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv2.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv2.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv2.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv3.conv.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for global_features.conv1.conv.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for global_features.conv1.conv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv1.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv1.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv1.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv1.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.conv.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for global_features.conv2.conv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.fc1.fc.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for global_features.fc1.fc.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for global_features.fc2.fc.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for global_features.fc2.fc.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for global_features.fc3.fc.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for global_features.fc3.fc.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1c555369e551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_one_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Projects\\hdrnet-pytorch\\utils.py\u001b[0m in \u001b[0;36mplot_one_image\u001b[1;34m(image_path, model_path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mhdrnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     reduced_transforms = transforms.Compose([\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\hdrnet\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1052\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HDRPointwiseNN:\n\tUnexpected key(s) in state_dict: \"low_level.conv5.conv.weight\", \"low_level.conv5.conv.bias\", \"low_level.conv5.bn.weight\", \"low_level.conv5.bn.bias\", \"low_level.conv5.bn.running_mean\", \"low_level.conv5.bn.running_var\", \"low_level.conv5.bn.num_batches_tracked\", \"low_level.conv6.conv.weight\", \"low_level.conv6.conv.bias\", \"low_level.conv6.bn.weight\", \"low_level.conv6.bn.bias\", \"low_level.conv6.bn.running_mean\", \"low_level.conv6.bn.running_var\", \"low_level.conv6.bn.num_batches_tracked\", \"global_features.conv3.conv.weight\", \"global_features.conv3.conv.bias\", \"global_features.conv3.bn.weight\", \"global_features.conv3.bn.bias\", \"global_features.conv3.bn.running_mean\", \"global_features.conv3.bn.running_var\", \"global_features.conv3.bn.num_batches_tracked\", \"global_features.conv4.conv.weight\", \"global_features.conv4.conv.bias\", \"global_features.conv4.bn.weight\", \"global_features.conv4.bn.bias\", \"global_features.conv4.bn.running_mean\", \"global_features.conv4.bn.running_var\", \"global_features.conv4.bn.num_batches_tracked\". \n\tsize mismatch for low_level.conv1.conv.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([8, 3, 3, 3]).\n\tsize mismatch for low_level.conv1.conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv1.bn.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv1.bn.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv1.bn.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv1.bn.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for low_level.conv2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 8, 3, 3]).\n\tsize mismatch for low_level.conv2.conv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv2.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv2.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv2.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv2.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for low_level.conv3.conv.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for global_features.conv1.conv.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for global_features.conv1.conv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv1.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv1.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv1.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv1.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.conv.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for global_features.conv2.conv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.conv2.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for global_features.fc1.fc.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for global_features.fc1.fc.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for global_features.fc2.fc.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for global_features.fc2.fc.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for global_features.fc3.fc.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for global_features.fc3.fc.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "plot_one_image(image_path,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_photos_path = 'E:\\Projects\\StyleTransfer\\justinho_dataset_processed\\small_original_reduced_size'\n",
    "target_photos_path = 'E:\\Projects\\StyleTransfer\\justinho_dataset_processed\\small_edited_reduced_size'\n",
    "\n",
    "dataset = HDRDataset(orig_photos_path,target_photos_path)\n",
    "dataset_length = len(dataset)\n",
    "train_length = math.ceil(.8 *dataset_length)\n",
    "val_length = dataset_length-train_length\n",
    "train_set, val_set = random_split(dataset, [train_length, val_length], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = 4)\n",
    "val_loader =  DataLoader(val_set, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch_of_images(val_loader,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
